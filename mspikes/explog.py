#!/usr/bin/env python
# -*- coding: iso-8859-1 -*-
"""
module for reading explog files. These files contain a bunch of very
essential meta-data generated during data acquisition by saber, in particular
time offsets for triggers and stimulus presentation
"""

import tables as t
import re, sys, os
    
__all__ = ['explog','readexplog']

class explog(object):
    """
    This class manages data associated with an explog file.  The
    text explog generated by saber is parsed into a number of tables
    which are stored in an hdf5 file for rapid access later.

    Several features of how saber stores data, and with extracellular
    physiology experiments in general determine the structure of this
    hdf5 file.
    
    Recordings are generally associated with a particular penetration
    and recording site.  

    Recording is assumed to be episodic; that is, recording on one
    or more channels is triggered at a given time.  Episodes do not
    need to be the same length, but all the channels are assumed to
    be triggered together and have the same length.

    Saber stores raw pcm data in pcm_seq2 files, which can have multiple
    entries.  Recordings are split across files if the size of the
    file exceeds some limit, and each channel has its own file.

    While recordings are being made, stimuli are often presented to the
    animal.  Saber records the times of stimulus presentation in an
    asynchronous manner, so there is no a priori match between an episode
    and a stimulus.  However, in my experiments a single stimulus
    is played in each episode, so it's possible to reconstruct (based on
    the times) which stimuli belong with which episode.
    """

    samplerate = 20

    def __init__(self, logfile, mode='r', pen=None, site=None):
        """
        Initialize the explog object with an hdf5 file. To
        generate an explog object from a raw saber explog, use
        the module-level function readexplog()
        """
        if isinstance(logfile, t.File):
            self.elog = logfile
        else:
            self.elog = t.openFile(logfile, mode)
        # set the current site
        if pen!=None and site!=None:
            self.site = (pen,site)
        else:
            fsite = self.sites[0]
            n,pen,site = fsite.split('_')
            self.site = (int(pen), int(site))

    def __del__(self):
        if hasattr(self, 'elog'):
            self.elog.close()

    def _get_site(self):
        return self._site

    def _set_site(self, site):
        self._site = (int(site[0]), int(site[1]))
        g = self._getgroup()  # this will throw an error if the site is not defined

    site = property(_get_site, _set_site, None, "The current recording site")

    @property
    def sites(self):
        """ Return a list of currently defined sites """
        return self.elog.root._g_listGroup()[0]

    def _getgroup(self):
        """ Returns the group for the current site """
        try:
            return self.elog.getNode('/site_%d_%d' % self.site)
        except t.NoSuchNodeError:
            raise ValueError, "The pen/site %d/%d has note been defined." % self.site

    def _gettable(self, tablename):
        """ Returns the table associated with the current site """
        return self.elog.getNode(self._getgroup(), tablename)

    def __iter__(self):
        """ Iterates through all the entries in the current site.  """
        table = self._gettable('entries')
        for r in table:
            yield r

    def __itersites(self):
        for sname in self.sites:
            b,pen,site = sname.split("_")
            self.site = (pen,site)
            yield self._getgroup()

    def getentry(self, abstime):
        """
        Looks up the record associated with a particular episode. If
        no records match the abstime, an empty recarray will be returned

        <abstime> - the time offset of the entry
        """
        table = self._gettable('entries')
        rnums = table.getWhereList('abstime==%d' % abstime)
        return table.readCoordinates(rnums)

    def getfiles(self, abstime=None):
        """
        Returns the file/entry pairs associated with a particular episode.
        """
        table = self._gettable('files')
        if abstime==None:
            return table[:]
        rnums = table.getWhereList('abstime==%d' % abstime)
        return table.readCoordinates(rnums)
       
    def getentrytimes(self, entry=None):
        """
        Every entry has an abstime associated with it (the start of the
        episode).  All the records (for each channel) associated with
        an entry will have the same abstime.  This method returns all
        the abstime values.
        """
        table = self._gettable('entries')
        atimes = table.col('abstime')
        if entry==None:
            return atimes
        else:
            return atimes[entry]

    def getstimulus(self, abstime):
        """
        Returns the stimulus or stimuli played in a particular episode
        """
        table = self._gettable('stimuli')
        rnum = table.getWhereList('entrytime==%d' % abstime)
        return table.readCoordinates(rnum)

    @property
    def channels(self):
        """  Returns all the channels defined in the explog  """
        table = self.elog.root.channels
        return [r['name'] for r in table]

    @property
    def nchannels(self):
        return len(self.channels)

    @property
    def nentries(self):
        """ Returns the number of entries in the currrent site """
        return len(self._gettable('entries'))

    @property
    def totentries(self):
        """ Returns the total number of entries in the file """
        currsite = self.site
        tot = 0
        for site in self.__itersites():
            tot += len(site.entries)
        self.site = currsite
        return tot

    @property
    def totstimuli(self):
        """ Returns the total number of stimuli in the file """
        currsite = self.site
        tot = 0
        for site in self.__itersites():
            tot += len(site.stimuli)
        self.site = currsite
        return tot

# end class explog

# regular expressions used in parsing the explogfile   
_reg_create = re.compile(r"'(?P<file>(?P<base>\S+)_\w+.pcm_seq2)' (?P<action>created|closed)")
_reg_triggeron = re.compile(r"_ON, (?P<chan>\S+):entry (?P<entry>\d+) \((?P<onset>\d+)\)")
_reg_triggeroff = re.compile(r"_OFF, (?P<chan>\S+):entry (?P<entry>\d+), wrote (?P<samples>\d+)")
_reg_stimulus = re.compile(r"stimulus: REL:(?P<rel>[\d\.]*) ABS:(?P<abs>\d*) NAME:'(?P<stim>\S*)'")
_reg_site = re.compile(r"site (?P<site>\d*)")
_reg_pen  = re.compile(r"pen (?P<pen>\d*)")


class Entries(t.IsDescription):
    """ Table for entries (episodes) """
    abstime  = t.UInt32Col(pos=0)
    duration = t.UInt32Col(pos=3)
    valid    = t.BoolCol(pos=4)

class Stimuli(t.IsDescription):
    abstime  = t.UInt32Col(pos=0)
    name     = t.StringCol(128, pos=1)
    entrytime= t.UInt32Col(pos=2)

class Files(t.IsDescription):
    """ This table is used to look up the files associated with an entry  """
    abstime  = t.UInt32Col(pos=0)
    channel  = t.UInt16Col(pos=1)
    filebase = t.StringCol(128,pos=2)
    entry    = t.UInt16Col(pos=3)

class Channels(t.IsDescription):
    name     = t.StringCol(64)


def readexplog(logfile, outfile, site_sort=False):
    """
    Parses episode information from the explog. Generates an h5
    file with the entry data in one table and the stimulus data in
    another.

    site_sort - if true, makes a directory for each pen/site and moves
                the files into that directory.

    Returns a new explog object
    """

    expdir = os.path.dirname(logfile)
    channels = []
    files  = {}
    triggers = {}
    
    currentpen   = 0
    currentsite  = 0
    currgrp      = None
    lastabs      = 0
    absoffset    = 0

    sitetmpl = 'site_%d_%d'

    fp = open(logfile)
    line_num = 0

    h5 = t.openFile(outfile, mode='w', title="parsed explog %s" % logfile,
                    filters=t.Filters(complevel=1, complib='zlib'))

    # channels are defined for the whole file
    chantable = h5.createTable(h5.root,'channels',Channels)
    

    # these functions should be called whenever a file is opened, as they will determine
    # how the entries for those files are stored in the hdf5 file
    def setsite(pen,site):
        # check for the site first
        sname = 'site_%d_%d' % (pen,site)
        sites = h5.root._g_listGroup()[0]
        h5.flush()
        if sname in sites:
            grp = h5.getNode('/',sname)
            entries = grp.entries
            stimuli = grp.stimuli
            ftbl = grp.files
        else:
            # create it and the tables if missing
            grp = h5.createGroup('/', sitetmpl % (pen, site))
            entries = h5.createTable(grp,'entries',Entries)
            stimuli = h5.createTable(grp,'stimuli',Stimuli)
            ftbl = h5.createTable(grp,'files',Files)
        return grp, stimuli, ftbl, entries

    # set up a dummy group for stimuli played before pen/site are called
    currgrp, stim_tbl, file_tbl, ent_tbl = setsite(currentpen, currentsite)
    stim_row = stim_tbl.row
    ent_row = ent_tbl.row
    file_row = file_tbl.row

    for line in fp:
        line_num += 1
        lstart = line[0:4]  # all the start codes are the same length

        if lstart == "FFFF":
            m1 = _reg_create.search(line)
            if m1:
                if m1.group('action')=='created':
                    # we try to locate the file either in the current directory
                    # or in a subdirectory site_<pen>_<site>
                    seqfile = m1.group('file')
                    subdir = currgrp._v_name  # returns the name of the group

                    if site_sort and os.path.exists(seqfile):
                        if not os.path.exists(subdir): os.mkdir(subdir)
                        print "%s -> %s" % (seqfile, subdir)
                        os.system("mv %s %s" % (seqfile, subdir))

                    if os.path.exists(os.path.join(subdir, seqfile)):
                        seqfile = os.path.join(subdir, seqfile)
                    files[m1.group('base')] = seqfile
                else:
                    files.pop(m1.group('base'))
            else:
                print "parse error: Unparseable FFFF line (%d): %s" % (line_num, line)

        # new pen or new site
        elif lstart == "IIII":
            m1 = _reg_pen.search(line)
            m2 = _reg_site.search(line)
            if m1:
                currentpen = int(m1.group('pen'))
            elif m2:
                currentsite = int(m2.group('site'))
            # note that creating a group for each time pen/site is typed
            # is wasteful, we can just check these groups later for whether
            # they have files and drop them
            currgrp, stim_tbl, file_tbl, ent_tbl = setsite(currentpen, currentsite)
            stim_row = stim_tbl.row
            ent_row = ent_tbl.row
            file_row = file_tbl.row

        # when saber quits or stop/starts, the abstime gets
        # reset. Since stimuli are matched with triggers by abstime,
        # this can result in stimuli getting assigned to episodes deep
        # in the past The workaround for this is to maintain an offset
        # that gets set to the most recent abstime whenever a quit
        # event is detected
        elif lstart == '%%%%':
            if line.rstrip().endswith('start'):
                absoffset = lastabs
            else:
                o = line.find('add')
                if o > 0:
                    cname = line[o+4:-1]
                    try:
                        channels.index(cname)
                    except ValueError:
                        channels.append(cname)


        # trigger lines: this is kind of tricky.  Need to make one
        # entry per trigger for the files table (i.e. for each
        # channel), and one entry per abstime for the entry table
        elif lstart == "TTTT":
            m1 = _reg_triggeron.search(line)
            if m1:
                # trigger on
                time_trig = int(m1.group('onset')) + absoffset
                triggers[(m1.group('chan'), int(m1.group('entry')))] = time_trig
                lastabs = time_trig
            else:
                m2 = _reg_triggeroff.search(line)
                if m2:
                    # trigger off
                    key = (m2.group('chan'), int(m2.group('entry')))
                    try:
                        trig_on = triggers[key]
                        n_samples = int(m2.group('samples'))
                        # check that we have a file defined in the files dict
                        if not files.has_key(key[0]):
                            print "parse error: found entry %s:%d but no file (line %d)" % \
                              (key + (line_num,))
                        else:
                            file_row['abstime'] = trig_on
                            file_row['filebase'] = files[key[0]]
                            file_row['channel'] = channels.index(key[0])
                            file_row['entry'] = key[1]
                            file_row.append()
                            triggers.pop(key)
                            # when we've emptied the triggers dictionary, it's time to write the
                            # entry record
                            if len(triggers)==0:
                                ent_row['abstime'] = trig_on
                                ent_row['duration'] = n_samples
                                ent_row['valid'] = True
                                ent_row.append()

                    except KeyError:
                        print "warning: found TRIG_OFF for %s:%s, but missed TRIG_ON (line %d)" % \
                              (key + (line_num,))
                    except IndexError:
                        print "Unable to parse channel number from %s (line %d)" % (key[0] % line_num)
                        triggers.remove(key)
                else:
                        print "parse error: Unparseable TTTT line (%d): %s" % (line_num, line)


        # stimulus lines
        elif lstart == "QQQQ":
            m1 = _reg_stimulus.search(line)
            if m1:
                time_stim_rel = float(m1.group('rel'))
                time_stim_abs = int(m1.group('abs')) + absoffset
                lastabs = time_stim_abs
                stimname = m1.group('stim')
                # is it only triggered stimuli that start with "File="?
                # I'm going to leave this as general as possible at some memory cost;
                # the untriggered stimuli will get discarded
                if stimname.startswith('File='):
                    stimname = stimname[5:]
                stim_row['abstime'] = time_stim_abs
                stim_row['name'] = stimname
                stim_row.append()
            else:
                print "parse error: Unparseable QQQQ line: %s" % line

    # done parsing file
    fp.close()

    ct_row = chantable.row
    for c in channels:
        ct_row['name'] = c
        ct_row.append()

    # do some cleanup
    h5.flush()
    for grpn in h5.root._g_listGroup()[0]:
        grp = h5.getNode('/',grpn)
        if grp._v_name=='/':
            continue
        # check for files; otherwise drop
        if grp.files.shape==(0,):
            grp._f_remove(recursive=True)
        else:
            assignstimuli(grp)

    return explog(h5)

# end readexplog()

def assignstimuli(group, cull_unrecorded=True):
    """
    Sets the stimulus entrytime values to match the corresponding
    entries in the entry table.

    <group> is any group containing the 'entries' and 'stimuli' tables

    If cull_unrecorded is true, the stimulus records that don't
    correspond to any recorded entry are removed from the database
    """
    entries = group.entries
    stimuli = group.stimuli

    start_times = entries.cols.abstime[:]
    stop_times = entries.cols.duration[:] + start_times

    i = 0
    for stimulus in stimuli:
        atime = stimulus['abstime']
        while i < len(stop_times):
            if atime < start_times[i]:
                stimulus['entrytime'] = 0L
                stimulus.update()
                break
            elif atime < stop_times[i]:
                stimulus['entrytime'] = long(start_times[i])
                stimulus.update()
                break
            else:
                i += 1

    stimuli.flush()
    
    if cull_unrecorded:
        # easiest just to copy the valid rows to a new table
        valid = stimuli.getWhereList('entrytime!=0')
        if len(valid)==0:
            print "Warning: no stimuli match %s, keeping old list" % group
        else:        
            stimuli.rename('oldstimuli')
            newstimuli = _maketable(group._v_file, group, 'stimuli', Stimuli)
            newstimuli.append(stimuli[:][valid])
            newstimuli.flush()
            stimuli.remove()

    group._f_flush()

def _maketable(file, base, name, descr):
    tbl = file.createTable(base, name, descr)
    #tbl.flavor = 'numpy'
    return tbl



if __name__=="__main__":

    import getopt

    if len(sys.argv) < 2:
        print """
        explog.py [-s] [-o <outfile>] <explog>
        
        Run this script to parse a text explog into an h5 file. Optionally
        (with -s) sort pcm_seq2 files into directories by pen/site. By default
        the output .h5 file is <explog>.explog.h5; use the -o flag to specify
        something else.
        
        """
        sys.exit(-1)

    opts,args = getopt.getopt(sys.argv[1:],"so:")

    site_sort = False
    if len(args)== 0:
        print "Must supply an explog file for input"
        sys.exit(-1)
    infile = args[0]
    outfile = infile + ".h5"

    for o,a in opts:
        if o=='-s':
            site_sort = True
        if o=="-o":
            outfile = a

    if os.path.exists(outfile):
        os.remove(outfile)
        
    z = readexplog(infile, outfile, site_sort)
