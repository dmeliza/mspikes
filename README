#-*- mode: org -*-
#+AUTHOR:    Dan Meliza
#+EMAIL:     dan||meliza.org
#+DATE: [2013-05-28 Tue]

*mspikes* is a general-purpose middleware tool for processing neural recordings.
It can read and write data in various formats and perform a range of filtering
and detection operations. You can use it to extract data from raw recordings for
spike sorting, and convert the results back into more generic formats. Support
for additional formats and signal processing algorithms can be supplied through
plugins.

*mspikes* uses a graph-based pipeline model.  Data is moved through the graph
from source to sink, passing through modules that transform the data.  Almost
any data processing task can be represented in this form.  Independent
operations can be easily parallelized for high performance in multi-core and
multi-node environments.

** Data model

Data in *mspikes* can either be sampled or event-based. Sampled data consist of
discrete measurements taken at fixed intervals (for example, a sound pressure
waveform or raw neural recording). Event-based data occur at discrete points in
time, but the intervals are not fixed. Examples of event-based data include
spike times and the associated waveforms, or the start and stop times of
stimulus presentation.  Data may be univariate or multivariate.  Sampled data
are represented by multi-dimensional arrays, with time along the first axis.
Event data are represented by by one-dimensional arrays of records, with the
time of the event stored in one of the fields.

Data are moved within *mspikes* in chunks. Chunks consist of a data array and
several fields that identify the source channel and the timebase. A timebase may
be continuous or discrete. In a continuous timebase, all time values in the
chunk are real values and have units of seconds.  In a discrete timebase, all
values are integers, and a sampling_rate attribute, with units of
samples/second is required.

+ channel :: the name of the channel
+ time :: the temporal offset of the data. Typically this value is referenced to
          the start of the experiment, but the only requirement is that all time
          values are internally consistent.
+ sampling_rate :: for discrete timebases, a positive real number indicating
                   the number of samples per second. For continuous timebases,
                   this property must be None.
+ data :: an array

** Graph model

Toolchains are defined in *mspikes* in terms of directed graphs of component
modules. Each node in the graph and its inputs are defined using a simple
syntax:

: node_name = node_class(sources, filters=None, **default_parameters)

=node_class= must be a concrete type (e.g., =arf_reader=). Some generic types
are available (e.g., =sampled_data_reader=) that will attempt to infer a more
specific type based on the file name or contents.

The =filters= parameter, if not =None=, must be a sequence with elements that
define what kinds of data packets are accepted. Currently only =event= and
=sampled= are accepted, but eventually function names may be allowed.

** Supported input formats

+ ARF/JILL :: ARF files (https://github.com/dmeliza/arf) created by jrecord,
              which is part of the JILL data acquisition suite
              (https://github.com/dmeliza/jill)
+ ARF/saber :: ARF files created by converting explog and pcm_seq2 files from
               saber, by Amish Dave

** Supported spike sorters

+ klusters


** Support output formats

+ ARF :: spike times can be stored in datasets linked to the original
         recordings, or in separate files
+ toelis :: https://github.com/dmeliza/toelis


** Requirements:

Python version 2.5 or higher; numpy version >=1.3 (for numerics);
scipy version >=0.5 (for FFT and linear algebra); and arf >=1.0.0, for
data IO.

Optional: matplotlib >= 0.90 is used by spike_view to plot spike
statistics and waveforms.  If this isn't installed, that script won't run.

** Installation:

In the mspikes directory, run python setup.py install

** Usage:

First, the raw saber data need to be converted to ARF format.  During
this process the data is annotated with metadata found in the saber
explog file.  The 'arfxplog' program that ships with arf is designed to
convert a saber experiment to ARf format:

$ arfxplog -v -a <animal> -e <experimenter> -T EXTRAC_HP -s <explog>

Giving arfxplog the '-s' option causes it to split the data into files
corresponding to recording site (pen/site commands in saber).
IMPORTANT: specify the data type as EXTRAC_HP if you want to see it in
mspike_view.

Second, inspect the data to determine which episodes and channels are
likely to contain single- or multiple-unit activity.  For recordings
in which the animal is awake it is usually desirable to exclude
episodes where it moved.  Single events can usually be sorted out, but
prolonged wiggling generates big clusters of noise that can
dramatically increase the RMS of the signal. Plot the RMS for each
episode using the following command:

$ mspike_view --stats <site.arf>

The bad episodes can be excluded at later stages based on RMS or
time. The 'mspike_view' program can also be used to view the raw spike
waveforms and determine an appropriate threshold for the window
discriminator. It usually makes sense to set the threshold in terms of
RMS units above the mean, since the DC offset and RMS noise level can
shift over the course of acquisition.  The threshold can usually be
set fairly low (around 4.5x RMS) because any single unit worth the
name will cluster out from the noise. Use the following command to
examine waveforms (restricting to channels of interest with the chan
argument):

$ mspike_view [--chan=<chan1>,<chan2>,...] <site.arf>

IMPORTANT NOTE: Channels are numbered starting from 0. Multiple
channels must be specified as a comma-separated list.

Third, extract spike times and, optionally, spike waveforms.  For
multi-unit activity or for extremely well isolated units, simple
threshold detection is sufficient.  To detect spikes and store the
event times in the ARF file, use the following command:

$ mspike_extract --simple [--chan=<c1>,<c2>,...] -r <thresh> [OPTIONS] <site.arf>

However, to isolate single units from noise and other units, it is
necessary to extract the waveforms and compute one or more features of
the spike waveforms.  Mspikes uses principal components as well as
several direct measures of spike shape.  The following command will
extract spike waveforms and measure features.

$ mspike_extract [--chan=<c1>,<c2>,...] -r <thresh> [OPTIONS] <site.arf>

This command will export data in a format readable by KlustaKwik (for
automated spike-sorting) and Klusters (for manual cluster cutting).
We recommend using KlustaKwik followed by Klusters to check clusters and
adjust as needed.  To automatically call KlustaKwik on the output of
mspike_extract, add the --kkwik flag.

Finally, collate events based on unit, episode, and stimulus with the
mspike_group tool:

$ mspike_group -a -t <site.arf>

The '-a' flag tells mspike_group to add event data to the original arf
file, grouped by unit and entry.  The '-t' flag tells it to further
organize the event data by stimulus, and output multi-repeat toe_lis
files for each stimulus.  Both flags are optional.  Additional options
are available to restrict the script to specific units, stimuli, and
epidodes.

** Copyright/Licence

Copyright (c) 2010-2013 C Daniel Meliza.

This Software is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the Free
Software Foundation; either version 3 of the License, or (at your option) any
later version.

This Software is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
this program (see COPYING); if not, see <http://www.gnu.org/licenses>.

